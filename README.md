# NYC Property Sales Dataset

A **data engineering project** analyzing New York City property sales over a 12-month period.  
Includes a complete **ETL pipeline**, **data parsing**, and **EDA** in Python.

---

## Overview

**Goals:**
- Extract, clean, and transform NYC property sales data  
- Load processed data into PostgreSQL and Parquet  
- Visualize and analyze sales trends  

**Dataset:**
- Source: NYC Department of Finance  
- [Google Drive Link](https://drive.google.com/drive/folders/1NhSdry2LAagL66Vec8ckH4ypVeg3Iwp6?usp=sharing)  
- Includes location, sale, and property classification details  
- Covers both full-building and unit sales  

---

## Structure

```
NYC-Property-Sales/
â”œâ”€â”€ api_example/                 # API integration examples
â”‚   â”œâ”€â”€ api_reader.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ etl/                         # ETL pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py               # Extract data from sources
â”‚   â”œâ”€â”€ transform.py             # Data cleaning and transformation
â”‚   â”œâ”€â”€ load.py                  # Load data to DB and Parquet
â”‚   â””â”€â”€ main.py                  # ETL entry point
â”œâ”€â”€ parse_example/               # Data parsing examples
â”‚   â”œâ”€â”€ data_parser.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ notebook/                    # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ src/                         # Project modules
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ pyproject.toml               # Project configuration (Poetry)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation
```

---

## Installation

```bash
conda create -n nyc_sales python=3.13 pip
conda activate nyc_sales
pip install pandas numpy matplotlib seaborn plotly jupyterlab gdown sqlalchemy psycopg2-binary pyarrow
```

---

## ETL Pipeline

## ðŸ”„ ETL Pipeline
```
etl/                         # ETL pipeline
â”œâ”€â”€ __init__.py              # package metadata
â”œâ”€â”€ extract.py                # Extract data from sources (CSV/Parquet/Google Drive)
â”œâ”€â”€ transform.py              # Data cleaning and type normalization
â”œâ”€â”€ load.py                   # Save data to Parquet and load <=100 rows into DB
â””â”€â”€ main.py                   # CLI orchestrator
```

**Data directories:**  
`data/raw/` and `data/processed/` are created at runtime.  

---

### Run the pipeline

**Minimal run (no DB load):**
```bash
python -m main --db_name homework --table_name bukalova --no-db
```
Loading into a DB table (100 rows):
```
python -m main --db_name homework --table_name bukalova
```

Pipeline stages

Extract: Reads the raw source and saves data/raw/nyc_sales.csv

Transform: Casts schema (dtypes, dates), cleans and normalizes data

Load: Writes data/processed/nyc_sales_clean.parquet and optionally inserts up to 100 rows into the specified database table

## EDA

Run interactive notebook:
```
jupyter notebook notebook/EDA.ipynb
```

**Includes:**
- Sales trends by borough
- Price distributions
- Correlation and anomaly detection

---

## Applications
- Real estate market & valuation analysis
- Price prediction models
- Spatial and time-series analysis

---

## Main Libraries
`pandas`, `numpy`, `matplotlib`, `seaborn`, `plotly`,  
`sqlalchemy`, `psycopg2-binary`, `pyarrow`, `jupyterlab`, `gdown`

